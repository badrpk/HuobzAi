import requests
import json
import time
import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# ‚úÖ Load AI Model
MODEL_NAME = "facebook/bart-large-cnn"  # Example model, adjust as needed
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)

# ‚úÖ Use CPU
device = torch.device("cpu")
model.to(device)

print("üöÄ Huobz AutoPilot AI Started...")

# ‚úÖ Fetch latest research papers (Example API call)
def fetch_web_knowledge():
    print("üåç Fetching latest web knowledge...")
    response = requests.get("https://api.example.com/research")  # Replace with actual API
    if response.status_code == 200:
        return response.json()
    else:
        print("‚ùå Failed to fetch research data")
        return []

# ‚úÖ Process and summarize fetched data
def summarize_text(text):
    inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
    inputs = {key: val.to(device) for key, val in inputs.items()}
    
    # ‚úÖ Fix min_length and max_length issue
    min_length = 10
    max_length = 100
    
    # ‚úÖ Ensure min_length < max_length dynamically
    max_length = max(min_length + 20, max_length)  

    summary_ids = model.generate(
        inputs["input_ids"],
        min_length=min_length,
        max_length=max_length,
        do_sample=True
    )
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# ‚úÖ Save summaries
def save_summary(data):
    with open("data/summary.json", "w") as f:
        json.dump(data, f)
    print("üíæ Summarized knowledge saved at data/summary.json")

# ‚úÖ Check AI backend server connection
def check_server(url):
    try:
        response = requests.get(url, timeout=5)
        return response.status_code == 200
    except requests.exceptions.ConnectionError:
        return False

# ‚úÖ Backend servers to check
backend_servers = [
    "http://192.168.1.100:5000",
    "http://192.168.1.101:5000",
    "http://192.168.1.102:5000"
]

# ‚úÖ Fetch & Process Data
research_papers = fetch_web_knowledge()

if research_papers:
    print(f"‚úÖ Fetched {len(research_papers)} research papers")
    
    summaries = []
    for paper in research_papers:
        summaries.append(summarize_text(paper["abstract"]))  # Adjust key as needed
    
    save_summary(summaries)

else:
    print("‚ùå No research papers fetched!")

# ‚úÖ Check AI Server Availability
server_connected = False

for server in backend_servers:
    if check_server(server):
        print(f"‚úÖ Connected to AI Server: {server}")
        server_connected = True
        break
    else:
        print(f"‚ùå Could not connect to {server}")

if not server_connected:
    print("‚è≥ Waiting for next cycle...")
    time.sleep(30)  # Retry after 30 seconds
